{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling for Italian Documents (BERTopic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bertopic hdbscan umap sentence_transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_or_download_embedding, get_embedding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Torch detects the runtime environment (are you running on a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 3.Word2vec embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained word embeddings (**Word2vec**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Word2Vec word embeddings...\")\n",
    "model = load_or_download_embedding(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained word embeddings (**Glove**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Loading Glove word embeddings...\")\n",
    "# model = load_or_download_embedding(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of word vectors in the model:\", len(model))\n",
    "print(\"Dimension of each word vector:\", model.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the word vectors (not very useful for humans 😊)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = \"deep\"\n",
    "word2 = \"learning\"\n",
    "\n",
    "embedding1 = get_embedding(word1, model)\n",
    "embedding2 = get_embedding(word2, model)\n",
    "\n",
    "print(f\"Embedding for '{word1}' (first 10 dimensions):\", embedding1[:10], \"...\")\n",
    "print(f\"Embedding for '{word2}' (first 10 dimensions):\", embedding2[:10], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get most similar words to a given word (**most_similar**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 🔍 How `most_similar` Works\n",
    "\n",
    "The method `most_similar(\"word\", topn=10)` returns the words that are most similar to the input word based on their embedding vectors.\n",
    "\n",
    "Internally, the model computes the **cosine similarity** between the vector of the given word and the vectors of all other words in the vocabulary:\n",
    "\n",
    "$\n",
    "\\text{similarity}(\\vec{v}_1, \\vec{v}_2) = \\frac{\\vec{v}_1 \\cdot \\vec{v}_2}{\\|\\vec{v}_1\\| \\cdot \\|\\vec{v}_2\\|}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ \\vec{v}_1 $ is the vector for the input word (e.g., `\"learning\"`)\n",
    "- $ \\vec{v}_2 $ is the vector for every other word in the vocabulary\n",
    "\n",
    "The method returns the top `n` words with the highest similarity scores.\n",
    "\n",
    "> 💡 This kind of similarity works well when the vectors have been trained on large corpora and reflect contextual word usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"learning\"\n",
    "\n",
    "similar_words = model.most_similar(word, topn=10)\n",
    "\n",
    "# Print results\n",
    "print(f\"Most similar words to {word}:\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📊 Let's visualize word vectors in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use dimensionality reduction to project high-dimensional word embeddings (usually 100–300 dimensions) down to 2D so we can plot them and visually explore semantic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 🌈 Option 1: Simple and fast — PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words_pca(model, words, highlight_words=None, figsize=(10, 7), title=\"PCA of Word Embeddings\"):\n",
    "    \"\"\"\n",
    "    Plot a 2D PCA projection of word embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - model: gensim KeyedVectors\n",
    "    - words: list of words to plot\n",
    "    - highlight_words: list of words to highlight (optional)\n",
    "    - figsize: tuple for figure size\n",
    "    - title: plot title\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in words if word in model.key_to_index]\n",
    "    filtered_words = [word for word in words if word in model.key_to_index]\n",
    "\n",
    "    if len(vectors) == 0:\n",
    "        print(\"No valid words found in the model.\")\n",
    "        return\n",
    "\n",
    "    # Reduce dimensions with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(vectors)\n",
    "\n",
    "    # Setup plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    for i, word in enumerate(filtered_words):\n",
    "        x, y = reduced[i]\n",
    "        is_highlighted = highlight_words and word in highlight_words\n",
    "        color = \"crimson\" if is_highlighted else \"steelblue\"\n",
    "        fontsize = 14 if is_highlighted else 12\n",
    "        plt.scatter(x, y, c=color, s=100 if is_highlighted else 60, edgecolors='k', linewidths=0.5)\n",
    "        plt.text(x + 0.02, y + 0.02, word, fontsize=fontsize, color=color)\n",
    "\n",
    "    plt.xlabel(\"PC1\", fontsize=13)\n",
    "    plt.ylabel(\"PC2\", fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"paris\", \"france\", \"rome\", \"italy\"]\n",
    "highlight_words = [\"paris\", \"france\", \"rome\", \"italy\"]\n",
    "plot_words_pca(model, words, highlight_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 🌈 Option 2: More powerful — t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better at preserving nonlinear relationships, but slower and more sensitive to parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words_tsne(model, words, perplexity=None, max_iter=1000, figsize=(10, 7), title=\"t-SNE of Word Embeddings\"):\n",
    "    \"\"\"\n",
    "    Plot a 2D t-SNE projection of word embeddings.\n",
    "\n",
    "    Automatically adjusts perplexity if not set or too high.\n",
    "\n",
    "    Parameters:\n",
    "    - model: gensim KeyedVectors\n",
    "    - words: list of words to plot\n",
    "    - perplexity: t-SNE perplexity (optional)\n",
    "    - max_iter: number of iterations\n",
    "    - figsize: figure size\n",
    "    - title: plot title\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in words if word in model.key_to_index]\n",
    "    filtered_words = [word for word in words if word in model.key_to_index]\n",
    "\n",
    "    if len(vectors) < 2:\n",
    "        print(\"⚠️ Need at least 2 valid words for t-SNE.\")\n",
    "        return\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "    # Set or adjust perplexity\n",
    "    max_perplexity = len(vectors) - 1\n",
    "    if perplexity is None or perplexity >= max_perplexity:\n",
    "        perplexity = max(2, min(30, max_perplexity))\n",
    "        print(f\"Using perplexity={perplexity}\")\n",
    "\n",
    "    # Run t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, max_iter=max_iter, random_state=42)\n",
    "    reduced = tsne.fit_transform(vectors)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    for i, word in enumerate(filtered_words):\n",
    "        x, y = reduced[i]\n",
    "        plt.scatter(x, y, c=\"darkorange\", s=70, edgecolors='k', linewidths=0.5)\n",
    "        plt.text(x + 1, y + 1, word, fontsize=12, color=\"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"paris\", \"france\", \"rome\", \"italy\"]\n",
    "plot_words_tsne(model, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Word Embeddings in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def plot_words_pca_3d(model, words, title=\"3D PCA of Word Embeddings\"):\n",
    "    \"\"\"\n",
    "    Interactive 3D PCA plot of word embeddings using Plotly.\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in words if word in model.key_to_index]\n",
    "    filtered_words = [word for word in words if word in model.key_to_index]\n",
    "\n",
    "    if len(vectors) < 3:\n",
    "        print(\"Need at least 3 valid words for 3D plot.\")\n",
    "        return\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "    pca = PCA(n_components=3)\n",
    "    reduced = pca.fit_transform(vectors)\n",
    "\n",
    "    x, y, z = reduced[:, 0], reduced[:, 1], reduced[:, 2]\n",
    "\n",
    "    trace = go.Scatter3d(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        mode='markers+text',\n",
    "        text=filtered_words,\n",
    "        textposition='top center',\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color='mediumturquoise',\n",
    "            line=dict(width=0.5, color='black')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        scene=dict(\n",
    "            xaxis_title='PC1',\n",
    "            yaxis_title='PC2',\n",
    "            zaxis_title='PC3'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"paris\", \"france\", \"rome\", \"italy\"]\n",
    "plot_words_pca_3d(model, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plurality direction\n",
    "plurality_vector = model[\"cats\"] - model[\"cat\"]\n",
    "plurality_unit = plurality_vector / norm(plurality_vector)\n",
    "\n",
    "# Word vectors\n",
    "puppy = model[\"puppy\"]\n",
    "puppies = model[\"puppies\"]\n",
    "\n",
    "# 1. Alignment with plurality direction\n",
    "cos_singular = np.dot(puppy, plurality_unit) / norm(puppy)\n",
    "cos_plural = np.dot(puppies, plurality_unit) / norm(puppies)\n",
    "\n",
    "# 2. Predict plural form\n",
    "puppies_tilde = puppy + plurality_vector\n",
    "\n",
    "# 3. Cosine similarity between predicted and real plural\n",
    "similarity_tilde = np.dot(puppies_tilde, puppies) / (norm(puppies_tilde) * norm(puppies))\n",
    "\n",
    "# Output\n",
    "print(f\"📐 Alignment with plurality direction:\")\n",
    "print(f\" - puppy     (singular): {cos_singular:.4f}\")\n",
    "print(f\" - puppies   (plural):   {cos_plural:.4f}\")\n",
    "print()\n",
    "print(f\"🔁 Cosine similarity:\")\n",
    "print(f\" - between 'puppies' and predicted 'puppies_tilde': {similarity_tilde:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. BERTopic Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality is nltk italian stopwords is very low, therefore we implement a method that import a list of stopwords in the **resources** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(it_path='resources/stopwords_it.txt', include_english=True):\n",
    "    \"\"\"\n",
    "    Loads a list of Italian stopwords from file and optionally adds English stopwords from NLTK.\n",
    "    \n",
    "    Parameters:\n",
    "    - it_path: path to the Italian stopwords file (one word per line)\n",
    "    - include_english: whether to include English stopwords from NLTK\n",
    "    \n",
    "    Returns:\n",
    "    - A list of unique stopwords\n",
    "    \"\"\"\n",
    "    # Load Italian stopwords from file\n",
    "    with open(it_path, 'r', encoding='utf-8-sig') as file:\n",
    "        stopwords_it = file.read().splitlines()\n",
    "    \n",
    "    # Optionally include English stopwords from NLTK\n",
    "    if include_english:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        stopwords_en = stopwords.words('english')\n",
    "    else:\n",
    "        stopwords_en = []\n",
    "\n",
    "    # Combine, remove duplicates and strip whitespace\n",
    "    stopwords_tot = set(word.strip().lower() for word in stopwords_it + stopwords_en if word.strip())\n",
    "    \n",
    "    return list(stopwords_tot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our custom method to load stopwords\n",
    "stop_words = load_stopwords()\n",
    "print(f\"Total stopwords loaded: {len(stop_words)}\")\n",
    "print(stop_words[:10])  # show a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset (stored in **data** folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to perform test on a larger dataset\n",
    "df = pd.read_csv(\"data/repubblica_sample.csv\")\n",
    "documents = df[\"full_text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the dataset\n",
    "In this step, each document is split into individual sentences using NLTK's sentence tokenizer, configured for the Italian language. This granularity helps improve the quality of topic modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent_tokenize(doc, language=\"italian\") for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the SentenceTransformer model\n",
    "\n",
    "We load a pre-trained multilingual model (paraphrase-multilingual-MiniLM-L12-v2) from the SentenceTransformers library. This model maps each sentence to a dense vector representation that captures its semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sentence embeddings\n",
    "We encode all tokenized sentences into high-dimensional vectors using the transformer model. These embeddings are the basis for clustering and topic extraction in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = sentence_model.encode(sentences, show_progress_bar=True, batch_size=128, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define vectorization, dimensionality reduction, and clustering models\n",
    "This step sets up the main components of the BERTopic pipeline:\n",
    "\n",
    "- **`CountVectorizer`**: extracts n-gram features (unigrams and bigrams) from the text, excluding stop words.  \n",
    "- **`UMAP`**: reduces the dimensionality of the sentence embeddings while preserving the local and global structure of the data.  \n",
    "- **`HDBSCAN`**: performs density-based clustering on the reduced embeddings to group similar sentences into coherent topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=list(stop_words), ngram_range=(1, 2))\n",
    "umap_model = UMAP(n_neighbors=50, n_components=5, metric=\"cosine\", min_dist=0.01, random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=1000, min_samples=1, cluster_selection_epsilon=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ Why do we use `CountVectorizer` if we already have embeddings?\n",
    "\n",
    "BERTopic separates **how topics are discovered** from **how they are described**:\n",
    "\n",
    "- 🧠 **Embeddings** (e.g., Sentence Transformers) are used to **cluster** similar documents based on semantic meaning.\n",
    "- 🧾 **CountVectorizer** is used to **extract frequent words/phrases** from the raw text within each cluster.\n",
    "- 📊 These token counts are then used in a **class-based TF-IDF (c-TF-IDF)** calculation to identify the most representative terms for each topic.\n",
    "\n",
    "In short:\n",
    "\n",
    "| Step               | Tool Used           | Purpose                                |\n",
    "|--------------------|---------------------|----------------------------------------|\n",
    "| Clustering         | Embeddings + UMAP + HDBSCAN | Group similar documents                |\n",
    "| Topic representation | CountVectorizer + c-TF-IDF | Describe what each cluster is about   |\n",
    "\n",
    "This two-step approach allows BERTopic to generate both **coherent clusters** and **interpretable topic labels**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the BERTopic model  \n",
    "We now initialize the `BERTopic` model using the previously defined components:\n",
    "- `UMAP` for dimensionality reduction  \n",
    "- `HDBSCAN` for clustering  \n",
    "- `CountVectorizer` for token extraction\n",
    "\n",
    "Then we call `.fit_transform()` on the list of sentences and their embeddings.  \n",
    "This performs the full pipeline: clustering + topic extraction + probability assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic_model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = bertopic_model.fit_transform(sentences, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📄 Retrieve and save topic information  \n",
    "We retrieve an overview of all identified topics using `get_topic_info()`.  \n",
    "This includes:\n",
    "- Topic ID  \n",
    "- Number of documents in each topic  \n",
    "- Most representative words (based on c-TF-IDF)\n",
    "\n",
    "We also save this table as a CSV file for further analysis or sharing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_topic = bertopic_model.get_topic_info()\n",
    "info_topic.to_csv('results/topic_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_topic.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧾 Get Detailed Document-Level Topic Assignments  \n",
    "We extract document-level metadata using `get_document_info()`, including:\n",
    "- The original sentence  \n",
    "- Its assigned topic  \n",
    "- The probability score  \n",
    "- The most representative topic words\n",
    "\n",
    "This is useful for validating topic assignments or for qualitative analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_info = bertopic_model.get_document_info(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🔧 Word Cloud by Topic Function  \n",
    "This function allows you to input a `topic_id` and generate a word cloud using the representative words from BERTopic's output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_wordcloud(topic_id, doc_info, max_words=100):\n",
    "    \"\"\"\n",
    "    Generate and display a word cloud for a specific topic ID using BERTopic document info.\n",
    "\n",
    "    Parameters:\n",
    "    - topic_id (int): The topic number to visualize.\n",
    "    - doc_info (DataFrame): Output from bertopic_model.get_document_info().\n",
    "    - max_words (int): Maximum number of words to include in the word cloud.\n",
    "    \"\"\"\n",
    "    if topic_id not in doc_info[\"Topic\"].unique():\n",
    "        print(f\"Topic {topic_id} not found.\")\n",
    "        return\n",
    "\n",
    "    # Collect all representative words for the selected topic\n",
    "    topic_words = doc_info[doc_info[\"Topic\"] == topic_id][\"Representative Words\"]\n",
    "    text = \" \".join(topic_words)\n",
    "\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, max_words=max_words, background_color=\"white\", colormap=\"tab10\").generate(text)\n",
    "\n",
    "    # Plot it\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for Topic {topic_id}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word cloud for topic 2\n",
    "plot_topic_wordcloud(topic_id=2, doc_info=doc_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
