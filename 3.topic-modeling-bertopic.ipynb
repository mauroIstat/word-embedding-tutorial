{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling for Italian Documents (BERTopic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_or_download_embedding, get_embedding\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 3.Word2vec embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained word embeddings (**Word2vec**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Word2Vec word embeddings...\")\n",
    "model = load_or_download_embedding(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to load pre-trained word embeddings (**Glove**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove word embeddings...\n",
      "📦 Loading model from models\\glove-wiki-gigaword-50.model\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Glove word embeddings...\")\n",
    "model = load_or_download_embedding(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word vectors in the model: 400000\n",
      "Dimension of each word vector: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of word vectors in the model:\", len(model))\n",
    "print(\"Dimension of each word vector:\", model.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the word vectors (not very useful for humans 😊)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'deep' (first 10 dimensions): [ 0.31445   1.2024    0.066651 -0.20096  -0.049636  0.66882  -0.049386\n",
      "  0.44174   0.1799   -0.10196 ] ...\n",
      "Embedding for 'learning' (first 10 dimensions): [ 0.20461   0.48659  -0.55308  -0.27019   0.26336   0.15751  -0.28994\n",
      " -0.51824   0.051829  0.36225 ] ...\n"
     ]
    }
   ],
   "source": [
    "word1 = \"deep\"\n",
    "word2 = \"learning\"\n",
    "\n",
    "embedding1 = get_embedding(word1, model)\n",
    "embedding2 = get_embedding(word2, model)\n",
    "\n",
    "print(f\"Embedding for '{word1}' (first 10 dimensions):\", embedding1[:10], \"...\")\n",
    "print(f\"Embedding for '{word2}' (first 10 dimensions):\", embedding2[:10], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get most similar words to a given word (**most_similar**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 🔍 How `most_similar` Works\n",
    "\n",
    "The method `most_similar(\"word\", topn=10)` returns the words that are most similar to the input word based on their embedding vectors.\n",
    "\n",
    "Internally, the model computes the **cosine similarity** between the vector of the given word and the vectors of all other words in the vocabulary:\n",
    "\n",
    "$\n",
    "\\text{similarity}(\\vec{v}_1, \\vec{v}_2) = \\frac{\\vec{v}_1 \\cdot \\vec{v}_2}{\\|\\vec{v}_1\\| \\cdot \\|\\vec{v}_2\\|}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ \\vec{v}_1 $ is the vector for the input word (e.g., `\"learning\"`)\n",
    "- $ \\vec{v}_2 $ is the vector for every other word in the vocabulary\n",
    "\n",
    "The method returns the top `n` words with the highest similarity scores.\n",
    "\n",
    "> 💡 This kind of similarity works well when the vectors have been trained on large corpora and reflect contextual word usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"learning\"\n",
    "\n",
    "similar_words = model.most_similar(word, topn=10)\n",
    "\n",
    "# Print results\n",
    "print(f\"Most similar words to {word}:\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📊 Let's visualize word vectors in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use dimensionality reduction to project high-dimensional word embeddings (usually 100–300 dimensions) down to 2D so we can plot them and visually explore semantic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ✅ Option 1: Simple and fast — PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words_pca(model, words, highlight_words=None, figsize=(10, 7), title=\"PCA of Word Embeddings\"):\n",
    "    \"\"\"\n",
    "    Plot a 2D PCA projection of word embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - model: gensim KeyedVectors\n",
    "    - words: list of words to plot\n",
    "    - highlight_words: list of words to highlight (optional)\n",
    "    - figsize: tuple for figure size\n",
    "    - title: plot title\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in words if word in model.key_to_index]\n",
    "    filtered_words = [word for word in words if word in model.key_to_index]\n",
    "\n",
    "    if len(vectors) == 0:\n",
    "        print(\"No valid words found in the model.\")\n",
    "        return\n",
    "\n",
    "    # Reduce dimensions with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(vectors)\n",
    "\n",
    "    # Setup plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    for i, word in enumerate(filtered_words):\n",
    "        x, y = reduced[i]\n",
    "        is_highlighted = highlight_words and word in highlight_words\n",
    "        color = \"crimson\" if is_highlighted else \"steelblue\"\n",
    "        fontsize = 14 if is_highlighted else 12\n",
    "        plt.scatter(x, y, c=color, s=100 if is_highlighted else 60, edgecolors='k', linewidths=0.5)\n",
    "        plt.text(x + 0.02, y + 0.02, word, fontsize=fontsize, color=color)\n",
    "\n",
    "    plt.xlabel(\"PC1\", fontsize=13)\n",
    "    plt.ylabel(\"PC2\", fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"paris\", \"france\", \"rome\", \"italy\"]\n",
    "highlight_words = [\"paris\", \"france\", \"rome\", \"italy\"]\n",
    "plot_words_pca(model, words, highlight_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 🌈 Option 2: More powerful — t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better at preserving nonlinear relationships, but slower and more sensitive to parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words_tsne(model, words, perplexity=None, max_iter=1000, figsize=(10, 7), title=\"t-SNE of Word Embeddings\"):\n",
    "    \"\"\"\n",
    "    Plot a 2D t-SNE projection of word embeddings.\n",
    "\n",
    "    Automatically adjusts perplexity if not set or too high.\n",
    "\n",
    "    Parameters:\n",
    "    - model: gensim KeyedVectors\n",
    "    - words: list of words to plot\n",
    "    - perplexity: t-SNE perplexity (optional)\n",
    "    - max_iter: number of iterations\n",
    "    - figsize: figure size\n",
    "    - title: plot title\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in words if word in model.key_to_index]\n",
    "    filtered_words = [word for word in words if word in model.key_to_index]\n",
    "\n",
    "    if len(vectors) < 2:\n",
    "        print(\"⚠️ Need at least 2 valid words for t-SNE.\")\n",
    "        return\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "    # Set or adjust perplexity\n",
    "    max_perplexity = len(vectors) - 1\n",
    "    if perplexity is None or perplexity >= max_perplexity:\n",
    "        perplexity = max(2, min(30, max_perplexity))\n",
    "        print(f\"Using perplexity={perplexity}\")\n",
    "\n",
    "    # Run t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, max_iter=max_iter, random_state=42)\n",
    "    reduced = tsne.fit_transform(vectors)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    for i, word in enumerate(filtered_words):\n",
    "        x, y = reduced[i]\n",
    "        plt.scatter(x, y, c=\"darkorange\", s=70, edgecolors='k', linewidths=0.5)\n",
    "        plt.text(x + 1, y + 1, word, fontsize=12, color=\"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"paris\", \"france\", \"rome\", \"italy\"]\n",
    "plot_words_tsne(model, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🌍 Plotting Word Embeddings in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def plot_words_pca_3d(model, words, title=\"3D PCA of Word Embeddings\"):\n",
    "    \"\"\"\n",
    "    Interactive 3D PCA plot of word embeddings using Plotly.\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in words if word in model.key_to_index]\n",
    "    filtered_words = [word for word in words if word in model.key_to_index]\n",
    "\n",
    "    if len(vectors) < 3:\n",
    "        print(\"Need at least 3 valid words for 3D plot.\")\n",
    "        return\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "    pca = PCA(n_components=3)\n",
    "    reduced = pca.fit_transform(vectors)\n",
    "\n",
    "    x, y, z = reduced[:, 0], reduced[:, 1], reduced[:, 2]\n",
    "\n",
    "    trace = go.Scatter3d(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        mode='markers+text',\n",
    "        text=filtered_words,\n",
    "        textposition='top center',\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color='mediumturquoise',\n",
    "            line=dict(width=0.5, color='black')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        scene=dict(\n",
    "            xaxis_title='PC1',\n",
    "            yaxis_title='PC2',\n",
    "            zaxis_title='PC3'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"paris\", \"france\", \"rome\", \"italy\"]\n",
    "plot_words_pca_3d(model, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plurality direction\n",
    "plurality_vector = model[\"cats\"] - model[\"cat\"]\n",
    "plurality_unit = plurality_vector / norm(plurality_vector)\n",
    "\n",
    "# Word vectors\n",
    "puppy = model[\"puppy\"]\n",
    "puppies = model[\"puppies\"]\n",
    "\n",
    "# 1. Alignment with plurality direction\n",
    "cos_singular = np.dot(puppy, plurality_unit) / norm(puppy)\n",
    "cos_plural = np.dot(puppies, plurality_unit) / norm(puppies)\n",
    "\n",
    "# 2. Predict plural form\n",
    "puppies_tilde = puppy + plurality_vector\n",
    "\n",
    "# 3. Cosine similarity between predicted and real plural\n",
    "similarity_tilde = np.dot(puppies_tilde, puppies) / (norm(puppies_tilde) * norm(puppies))\n",
    "\n",
    "# Output\n",
    "print(f\"📐 Alignment with plurality direction:\")\n",
    "print(f\" - puppy     (singular): {cos_singular:.4f}\")\n",
    "print(f\" - puppies   (plural):   {cos_plural:.4f}\")\n",
    "print()\n",
    "print(f\"🔁 Cosine similarity:\")\n",
    "print(f\" - between 'puppies' and predicted 'puppies_tilde': {similarity_tilde:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
