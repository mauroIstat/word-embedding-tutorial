{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling for Italian Documents (BERTopic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bertopic hdbscan umap sentence_transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_or_download_embedding, get_embedding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Torch detects the runtime environment (are you running on a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 3.Word2vec embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained word embeddings (**Word2vec**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Word2Vec word embeddings...\")\n",
    "model = load_or_download_embedding(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to load pre-trained word embeddings (**Glove**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Glove word embeddings...\")\n",
    "model = load_or_download_embedding(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of word vectors in the model:\", len(model))\n",
    "print(\"Dimension of each word vector:\", model.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the word vectors (not very useful for humans 😊)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = \"deep\"\n",
    "word2 = \"learning\"\n",
    "\n",
    "embedding1 = get_embedding(word1, model)\n",
    "embedding2 = get_embedding(word2, model)\n",
    "\n",
    "print(f\"Embedding for '{word1}' (first 10 dimensions):\", embedding1[:10], \"...\")\n",
    "print(f\"Embedding for '{word2}' (first 10 dimensions):\", embedding2[:10], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get most similar words to a given word (**most_similar**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 🔍 How `most_similar` Works\n",
    "\n",
    "The method `most_similar(\"word\", topn=10)` returns the words that are most similar to the input word based on their embedding vectors.\n",
    "\n",
    "Internally, the model computes the **cosine similarity** between the vector of the given word and the vectors of all other words in the vocabulary:\n",
    "\n",
    "$\n",
    "\\text{similarity}(\\vec{v}_1, \\vec{v}_2) = \\frac{\\vec{v}_1 \\cdot \\vec{v}_2}{\\|\\vec{v}_1\\| \\cdot \\|\\vec{v}_2\\|}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ \\vec{v}_1 $ is the vector for the input word (e.g., `\"learning\"`)\n",
    "- $ \\vec{v}_2 $ is the vector for every other word in the vocabulary\n",
    "\n",
    "The method returns the top `n` words with the highest similarity scores.\n",
    "\n",
    "> 💡 This kind of similarity works well when the vectors have been trained on large corpora and reflect contextual word usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"learning\"\n",
    "\n",
    "similar_words = model.most_similar(word, topn=10)\n",
    "\n",
    "# Print results\n",
    "print(f\"Most similar words to {word}:\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📊 Let's visualize word vectors in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use dimensionality reduction to project high-dimensional word embeddings (usually 100–300 dimensions) down to 2D so we can plot them and visually explore semantic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ✅ Option 1: Simple and fast — PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words_pca(model, words, highlight_words=None, figsize=(10, 7), title=\"PCA of Word Embeddings\"):\n",
    "    \"\"\"\n",
    "    Plot a 2D PCA projection of word embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - model: gensim KeyedVectors\n",
    "    - words: list of words to plot\n",
    "    - highlight_words: list of words to highlight (optional)\n",
    "    - figsize: tuple for figure size\n",
    "    - title: plot title\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in words if word in model.key_to_index]\n",
    "    filtered_words = [word for word in words if word in model.key_to_index]\n",
    "\n",
    "    if len(vectors) == 0:\n",
    "        print(\"No valid words found in the model.\")\n",
    "        return\n",
    "\n",
    "    # Reduce dimensions with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(vectors)\n",
    "\n",
    "    # Setup plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    for i, word in enumerate(filtered_words):\n",
    "        x, y = reduced[i]\n",
    "        is_highlighted = highlight_words and word in highlight_words\n",
    "        color = \"crimson\" if is_highlighted else \"steelblue\"\n",
    "        fontsize = 14 if is_highlighted else 12\n",
    "        plt.scatter(x, y, c=color, s=100 if is_highlighted else 60, edgecolors='k', linewidths=0.5)\n",
    "        plt.text(x + 0.02, y + 0.02, word, fontsize=fontsize, color=color)\n",
    "\n",
    "    plt.xlabel(\"PC1\", fontsize=13)\n",
    "    plt.ylabel(\"PC2\", fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"paris\", \"france\", \"rome\", \"italy\"]\n",
    "highlight_words = [\"paris\", \"france\", \"rome\", \"italy\"]\n",
    "plot_words_pca(model, words, highlight_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 🌈 Option 2: More powerful — t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better at preserving nonlinear relationships, but slower and more sensitive to parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words_tsne(model, words, perplexity=None, max_iter=1000, figsize=(10, 7), title=\"t-SNE of Word Embeddings\"):\n",
    "    \"\"\"\n",
    "    Plot a 2D t-SNE projection of word embeddings.\n",
    "\n",
    "    Automatically adjusts perplexity if not set or too high.\n",
    "\n",
    "    Parameters:\n",
    "    - model: gensim KeyedVectors\n",
    "    - words: list of words to plot\n",
    "    - perplexity: t-SNE perplexity (optional)\n",
    "    - max_iter: number of iterations\n",
    "    - figsize: figure size\n",
    "    - title: plot title\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in words if word in model.key_to_index]\n",
    "    filtered_words = [word for word in words if word in model.key_to_index]\n",
    "\n",
    "    if len(vectors) < 2:\n",
    "        print(\"⚠️ Need at least 2 valid words for t-SNE.\")\n",
    "        return\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "    # Set or adjust perplexity\n",
    "    max_perplexity = len(vectors) - 1\n",
    "    if perplexity is None or perplexity >= max_perplexity:\n",
    "        perplexity = max(2, min(30, max_perplexity))\n",
    "        print(f\"Using perplexity={perplexity}\")\n",
    "\n",
    "    # Run t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, max_iter=max_iter, random_state=42)\n",
    "    reduced = tsne.fit_transform(vectors)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    for i, word in enumerate(filtered_words):\n",
    "        x, y = reduced[i]\n",
    "        plt.scatter(x, y, c=\"darkorange\", s=70, edgecolors='k', linewidths=0.5)\n",
    "        plt.text(x + 1, y + 1, word, fontsize=12, color=\"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"paris\", \"france\", \"rome\", \"italy\"]\n",
    "plot_words_tsne(model, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🌍 Plotting Word Embeddings in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def plot_words_pca_3d(model, words, title=\"3D PCA of Word Embeddings\"):\n",
    "    \"\"\"\n",
    "    Interactive 3D PCA plot of word embeddings using Plotly.\n",
    "    \"\"\"\n",
    "    vectors = [model[word] for word in words if word in model.key_to_index]\n",
    "    filtered_words = [word for word in words if word in model.key_to_index]\n",
    "\n",
    "    if len(vectors) < 3:\n",
    "        print(\"Need at least 3 valid words for 3D plot.\")\n",
    "        return\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "    pca = PCA(n_components=3)\n",
    "    reduced = pca.fit_transform(vectors)\n",
    "\n",
    "    x, y, z = reduced[:, 0], reduced[:, 1], reduced[:, 2]\n",
    "\n",
    "    trace = go.Scatter3d(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        mode='markers+text',\n",
    "        text=filtered_words,\n",
    "        textposition='top center',\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color='mediumturquoise',\n",
    "            line=dict(width=0.5, color='black')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        scene=dict(\n",
    "            xaxis_title='PC1',\n",
    "            yaxis_title='PC2',\n",
    "            zaxis_title='PC3'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"paris\", \"france\", \"rome\", \"italy\"]\n",
    "plot_words_pca_3d(model, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plurality direction\n",
    "plurality_vector = model[\"cats\"] - model[\"cat\"]\n",
    "plurality_unit = plurality_vector / norm(plurality_vector)\n",
    "\n",
    "# Word vectors\n",
    "puppy = model[\"puppy\"]\n",
    "puppies = model[\"puppies\"]\n",
    "\n",
    "# 1. Alignment with plurality direction\n",
    "cos_singular = np.dot(puppy, plurality_unit) / norm(puppy)\n",
    "cos_plural = np.dot(puppies, plurality_unit) / norm(puppies)\n",
    "\n",
    "# 2. Predict plural form\n",
    "puppies_tilde = puppy + plurality_vector\n",
    "\n",
    "# 3. Cosine similarity between predicted and real plural\n",
    "similarity_tilde = np.dot(puppies_tilde, puppies) / (norm(puppies_tilde) * norm(puppies))\n",
    "\n",
    "# Output\n",
    "print(f\"📐 Alignment with plurality direction:\")\n",
    "print(f\" - puppy     (singular): {cos_singular:.4f}\")\n",
    "print(f\" - puppies   (plural):   {cos_plural:.4f}\")\n",
    "print()\n",
    "print(f\"🔁 Cosine similarity:\")\n",
    "print(f\" - between 'puppies' and predicted 'puppies_tilde': {similarity_tilde:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality is nltk italian stopwords is very low, therefore we implement a method that import a list of stopwords in the **resources** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(it_path='resources/stopwords_it.txt', include_english=True):\n",
    "    \"\"\"\n",
    "    Loads a list of Italian stopwords from file and optionally adds English stopwords from NLTK.\n",
    "    \n",
    "    Parameters:\n",
    "    - it_path: path to the Italian stopwords file (one word per line)\n",
    "    - include_english: whether to include English stopwords from NLTK\n",
    "    \n",
    "    Returns:\n",
    "    - A list of unique stopwords\n",
    "    \"\"\"\n",
    "    # Load Italian stopwords from file\n",
    "    with open(it_path, 'r', encoding='utf-8-sig') as file:\n",
    "        stopwords_it = file.read().splitlines()\n",
    "    \n",
    "    # Optionally include English stopwords from NLTK\n",
    "    if include_english:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        stopwords_en = stopwords.words('english')\n",
    "    else:\n",
    "        stopwords_en = []\n",
    "\n",
    "    # Combine, remove duplicates and strip whitespace\n",
    "    stopwords_tot = set(word.strip().lower() for word in stopwords_it + stopwords_en if word.strip())\n",
    "    \n",
    "    return list(stopwords_tot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. BERTopic Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset (stored in **data** folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to perform test on a larger dataset\n",
    "df = pd.read_csv(\"data/repubblica_sample.csv\")\n",
    "documents = df[\"full_text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent_tokenize(doc, language=\"italian\") for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = sentence_model.encode(sentences, show_progress_bar=True, batch_size=128, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our custom method to load stopwords\n",
    "stop_words = load_stopwords()\n",
    "print(f\"Total stopwords loaded: {len(stop_words)}\")\n",
    "print(stop_words[:10])  # show a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=list(stop_words), ngram_range=(1, 2))\n",
    "umap_model = UMAP(n_neighbors=50, n_components=5, metric=\"cosine\", min_dist=0.01, random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=1000, min_samples=1, cluster_selection_epsilon=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic_model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = bertopic_model.fit_transform(sentences, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_topic = bertopic_model.get_topic_info()\n",
    "info_topic.to_csv('results/topic_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic_model.get_document_info(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
