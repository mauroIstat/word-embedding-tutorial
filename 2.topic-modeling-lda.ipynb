{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLKT resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a simple test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"I love deep learning and natural language processing.\",\n",
    "    \"Natural language models are fascinating.\",\n",
    "    \"Topic modeling helps discover themes in text.\",\n",
    "    \"Machine learning enables automatic topic discovery.\",\n",
    "    \"Neural networks learn embeddings from data.\",\n",
    "    \"Artificial Intelligence is transforming industries.\",\n",
    "    \"Text analysis techniques improve information retrieval.\",\n",
    "    \"Large language models power chatbots and assistants.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Simple Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization + lowercase + remove stopwords and punctuation\n",
    "def preprocess(doc):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    return [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Create dictionary and BoW corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opzionalmente: elenco di tutte le parole nel vocabolario\n",
    "vocab = [dictionary[i] for i in range(len(dictionary))]\n",
    "\n",
    "# Matrice documenti-parole\n",
    "bow_matrix = []\n",
    "for doc_bow in corpus:\n",
    "    word_freq = dict(doc_bow)\n",
    "    row = [word_freq.get(i, 0) for i in range(len(dictionary))]\n",
    "    bow_matrix.append(row)\n",
    "\n",
    "df_bow = pd.DataFrame(bow_matrix, columns=vocab)\n",
    "df_bow.index = [f'Doc {i+1}' for i in range(len(documents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Bag-of-Words** model is one of the simplest ways to represent text numerically. It ignores grammar and word order and focuses only on word occurrence.\n",
    "\n",
    "##### **What is it?**\n",
    "- Each document is treated as a \"bag\" of individual words.\n",
    "- A vocabulary is built from all the unique words in the corpus.\n",
    "- Each document is then represented as a vector counting how many times each word from the vocabulary appears.\n",
    "\n",
    "This results in a **document-term matrix**:\n",
    "- Each row corresponds to a document.\n",
    "- Each column corresponds to a word from the vocabulary.\n",
    "- Each cell contains the count of the word in that document.\n",
    "\n",
    "Although simple, BoW has limitations:\n",
    "- It does not consider word order or context.\n",
    "- It can result in very high-dimensional and sparse data.\n",
    "\n",
    "##### **Simple Example**\n",
    "Let's say we have two short documents:\n",
    "\n",
    "- Document 1: \"I love NLP\"\n",
    "- Document 2: \"I love machine learning\"\n",
    "\n",
    "The combined vocabulary is: `[I, love, NLP, machine, learning]`\n",
    "\n",
    "We can represent each document as a vector of word counts:\n",
    "\n",
    "| Document | I | love | NLP | machine | learning |\n",
    "|----------|---|------|-----|---------|----------|\n",
    "| Doc 1    | 1 | 1    | 1   | 0       | 0        |\n",
    "| Doc 2    | 1 | 1    | 0   | 1       | 1        |\n",
    "\n",
    "This matrix shows how many times each word appears in each document. No word order is preserved.\n",
    "\n",
    "Still, it‚Äôs a foundational method and helps build intuition for more sophisticated approaches like TF-IDF and word embeddings.\n",
    "\n",
    "##### üõ†Ô∏è **Code Example**\n",
    "\n",
    "The code block below uses `CountVectorizer` from `sklearn` to create the BoW matrix and displays it as a Pandas DataFrame for readability.\n",
    "\n",
    "This block creates a Bag-of-Words (BoW) representation of our corpus:\n",
    "- CountVectorizer transforms the documents into a matrix (documents x words).\n",
    "- Each element in the matrix represents how many times a word appears in a document.\n",
    "- The 'fit_transform' function builds the vocabulary and generates the counts.\n",
    "- Finally, we convert the matrix into a Pandas DataFrame for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(documents)\n",
    "pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_bow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
