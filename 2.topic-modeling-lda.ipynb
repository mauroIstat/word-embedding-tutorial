{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling for Italian Documents (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download NLKT resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality is nltk italian stopwords is very low, therefore we implement a method that import a list of stopwords in the **resources** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(it_path='resources/stopwords_it.txt', include_english=True):\n",
    "    \"\"\"\n",
    "    Loads a list of Italian stopwords from file and optionally adds English stopwords from NLTK.\n",
    "    \n",
    "    Parameters:\n",
    "    - it_path: path to the Italian stopwords file (one word per line)\n",
    "    - include_english: whether to include English stopwords from NLTK\n",
    "    \n",
    "    Returns:\n",
    "    - A list of unique stopwords\n",
    "    \"\"\"\n",
    "    # Load Italian stopwords from file\n",
    "    with open(it_path, 'r', encoding='utf-8-sig') as file:\n",
    "        stopwords_it = file.read().splitlines()\n",
    "    \n",
    "    # Optionally include English stopwords from NLTK\n",
    "    if include_english:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        stopwords_en = stopwords.words('english')\n",
    "    else:\n",
    "        stopwords_en = []\n",
    "\n",
    "    # Combine, remove duplicates and strip whitespace\n",
    "    stopwords_tot = set(word.strip().lower() for word in stopwords_it + stopwords_en if word.strip())\n",
    "    \n",
    "    return list(stopwords_tot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nlkt stopwords\n",
    "# stop_words = set(stopwords.words('italian'))\n",
    "\n",
    "# Use our custom method to load stopwords\n",
    "stop_words = load_stopwords()\n",
    "print(f\"Total stopwords loaded: {len(stop_words)}\")\n",
    "print(stop_words[:10])  # show a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a simple test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Amo il deep learning e l'elaborazione del linguaggio naturale.\",\n",
    "    \"I modelli di linguaggio naturale sono affascinanti.\",\n",
    "    \"Il topic modeling aiuta a scoprire i temi nei testi.\",\n",
    "    \"Il machine learning consente la scoperta automatica degli argomenti.\",\n",
    "    \"Le reti neurali apprendono rappresentazioni dai dati.\",\n",
    "    \"L'intelligenza artificiale sta trasformando le industrie.\",\n",
    "    \"Le tecniche di analisi del testo migliorano il recupero delle informazioni.\",\n",
    "    \"I modelli linguistici di grandi dimensioni alimentano chatbot e assistenti.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to perform test on a larger dataset\n",
    "df = pd.read_csv(\"data/repubblica_sample.csv\")\n",
    "documents = df[\"full_text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ 3.Gensim: creating dictionary and corpus\n",
    "\n",
    "Documents must be **tokenized** (lists of words) and represented as **bag-of-words** in the format (word_id, count).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Preprocessing and **Tokenization** (we could do much more advances stuff....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization + lowercase + remove stopwords and punctuation\n",
    "def preprocess(doc):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    return [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a dictionary from the processed documents\n",
    "# The dictionary maps each unique word in the entire corpus to a unique integer ID\n",
    "# For example, 'linguaggio' might be assigned ID 0, 'modello' might be ID 1, and so on\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Step 2: Create the corpus in Bag-of-Words (BoW) format\n",
    "# For each document, we generate a list of tuples: (word_id, word_count)\n",
    "# This means we‚Äôre counting how many times each word (by its ID) appears in the document\n",
    "# Example output for a document might be: [(0, 2), (3, 1)]\n",
    "# ‚Üí word with ID 0 appears 2 times, word with ID 3 appears once\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dictionary\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the BoW corpus\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üõ†Ô∏è Define a function to convert gensim_corpus to a dataframe (**gensim_corpus_to_dataframe**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_corpus_to_dataframe(corpus, dictionary, doc_labels=None):\n",
    "    \"\"\"\n",
    "    Converts a Gensim corpus into a readable pandas DataFrame (BoW matrix).\n",
    "    \n",
    "    Parameters:\n",
    "    - corpus: list of documents in Gensim BoW format [(word_id, count), ...]\n",
    "    - dictionary: Gensim Dictionary object mapping word IDs to words\n",
    "    - doc_labels: optional list of labels for the rows (e.g., ['Doc 1', 'Doc 2', ...])\n",
    "    \n",
    "    Returns:\n",
    "    - A pandas DataFrame where rows = documents, columns = words, values = word counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create vocabulary list from dictionary (index = word IDs)\n",
    "    vocab = [dictionary[i] for i in range(len(dictionary))]\n",
    "    \n",
    "    # Reconstruct BoW matrix\n",
    "    bow_matrix = []\n",
    "    for doc_bow in corpus:\n",
    "        word_freq = dict(doc_bow)\n",
    "        row = [word_freq.get(i, 0) for i in range(len(dictionary))]\n",
    "        bow_matrix.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_bow = pd.DataFrame(bow_matrix, columns=vocab)\n",
    "    \n",
    "    # Optional: set custom document labels\n",
    "    if doc_labels is None:\n",
    "        df_bow.index = [f'Doc {i+1}' for i in range(len(corpus))]\n",
    "    else:\n",
    "        df_bow.index = doc_labels\n",
    "\n",
    "    return df_bow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow = gensim_corpus_to_dataframe(corpus, dictionary, doc_labels=[f'Doc {i+1}' for i in range(len(documents))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ 4.Scikit-learn: creating the Bag-of-Words matrix\n",
    "\n",
    "Documents must be converted into **strings** (not token lists), then transformed into a **document-term matrix** using `CountVectorizer`,  where each row represents a document and each column a word, with values as word counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = [' '.join(doc) for doc in processed_docs]  # join tokens back into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the CountVectorizer\n",
    "# This tool will convert our text data into a matrix of token counts (Bag of Words)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 2: Fit the vectorizer to our documents and transform them into a sparse matrix\n",
    "# Note: processed_docs must be a list of strings (not token lists), like ['I love AI', 'AI is cool']\n",
    "X_bow = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "# Step 3: Convert the sparse matrix to a dense array and wrap it in a pandas DataFrame\n",
    "# Each row = a document, each column = a word, each cell = how many times the word appears in the document\n",
    "pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the content of the sparse matrix\n",
    "X_bow[1].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train an LDA model with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Step 1: Set number of topics (e.g. 3, but tune this!)\n",
    "num_topics = 5\n",
    "\n",
    "# Step 2: Train the LDA model\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,              # BoW representation of your documents\n",
    "    id2word=dictionary,         # Mapping from IDs to words\n",
    "    num_topics=num_topics,      # Number of latent topics\n",
    "    random_state=42,            # For reproducibility\n",
    "    passes=10,                  # Number of passes through the corpus during training\n",
    "    alpha='auto',               # Automatically learn the Dirichlet prior\n",
    "    per_word_topics=True        # Output word-level topic assignments (optional)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîç Inspect the discovered topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top words for each topic\n",
    "for i, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {i + 1}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ Predict topic(s) for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distribution for a specific document (e.g. doc 0)\n",
    "doc_topics = lda_model.get_document_topics(corpus[0])\n",
    "print(doc_topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display topics using **pyLDAvis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the visualization\n",
    "lda_display = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topics\n",
    "pyLDAvis.display(lda_display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
