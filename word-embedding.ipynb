{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Words to Vectors: A Journey into Text Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_or_download_embedding, get_embedding\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a simple test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"I love deep learning and natural language processing.\",\n",
    "    \"Natural language models are fascinating.\",\n",
    "    \"Topic modeling helps to discover themes in documents.\",\n",
    "    \"Machine learning enables automatic topic discovery.\",\n",
    "    \"Neural networks learn embeddings from data.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Bag-of-Words** model is one of the simplest ways to represent text numerically. It ignores grammar and word order and focuses only on word occurrence.\n",
    "\n",
    "##### **What is it?**\n",
    "- Each document is treated as a \"bag\" of individual words.\n",
    "- A vocabulary is built from all the unique words in the corpus.\n",
    "- Each document is then represented as a vector counting how many times each word from the vocabulary appears.\n",
    "\n",
    "This results in a **document-term matrix**:\n",
    "- Each row corresponds to a document.\n",
    "- Each column corresponds to a word from the vocabulary.\n",
    "- Each cell contains the count of the word in that document.\n",
    "\n",
    "Although simple, BoW has limitations:\n",
    "- It does not consider word order or context.\n",
    "- It can result in very high-dimensional and sparse data.\n",
    "\n",
    "##### **Simple Example**\n",
    "Let's say we have two short documents:\n",
    "\n",
    "- Document 1: \"I love NLP\"\n",
    "- Document 2: \"I love machine learning\"\n",
    "\n",
    "The combined vocabulary is: `[I, love, NLP, machine, learning]`\n",
    "\n",
    "We can represent each document as a vector of word counts:\n",
    "\n",
    "| Document | I | love | NLP | machine | learning |\n",
    "|----------|---|------|-----|---------|----------|\n",
    "| Doc 1    | 1 | 1    | 1   | 0       | 0        |\n",
    "| Doc 2    | 1 | 1    | 0   | 1       | 1        |\n",
    "\n",
    "This matrix shows how many times each word appears in each document. No word order is preserved.\n",
    "\n",
    "Still, it’s a foundational method and helps build intuition for more sophisticated approaches like TF-IDF and word embeddings.\n",
    "\n",
    "##### 🛠️ **Code Example**\n",
    "\n",
    "The code block below uses `CountVectorizer` from `sklearn` to create the BoW matrix and displays it as a Pandas DataFrame for readability.\n",
    "\n",
    "This block creates a Bag-of-Words (BoW) representation of our corpus:\n",
    "- CountVectorizer transforms the documents into a matrix (documents x words).\n",
    "- Each element in the matrix represents how many times a word appears in a document.\n",
    "- The 'fit_transform' function builds the vocabulary and generates the counts.\n",
    "- Finally, we convert the matrix into a Pandas DataFrame for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(documents)\n",
    "pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TF-IDF (Term Frequency–Inverse Document Frequency) Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is an improvement over the Bag-of-Words model. While BoW only counts word frequency, **TF-IDF balances frequency with uniqueness**, reducing the weight of common words that appear in many documents.\n",
    "\n",
    "##### **What is it?**\n",
    "- **Term Frequency (TF)** measures how often a word appears in a specific document.\n",
    "- **Inverse Document Frequency (IDF)** downweights words that appear in many documents.\n",
    "- The product **TF × IDF** gives more importance to words that are frequent in a document but rare across the corpus.\n",
    "\n",
    "The result is a **weighted document-term matrix** that emphasizes more informative words.\n",
    "\n",
    "TF-IDF(w, d, D) = TF(w, d) × IDF(w, D)\n",
    "\n",
    "Where:\n",
    "- **TF(w, d)** is the term frequency of word *w* in document *d*:\n",
    "  > TF(w, d) = (Number of times *w* appears in *d*) / (Total words in *d*)\n",
    "\n",
    "- **IDF(w, D)** is the inverse document frequency of *w* in the full corpus *D*:\n",
    "  > IDF(w, D) = log[(1 + N) / (1 + DF(w))] + 1\n",
    "\n",
    "  where:\n",
    "  - *N* is the total number of documents\n",
    "  - *DF(w)* is the number of documents containing the word *w*\n",
    "\n",
    "📌 This helps to penalize very common words (like \"the\", \"and\", \"is\") and give more weight to words that are specific to a document.\n",
    "\n",
    "##### 📋 TF-IDF Table Example\n",
    "\n",
    "Let’s use the same two documents:\n",
    "\n",
    "- Document 1: \"I love NLP\"  \n",
    "- Document 2: \"I love machine learning\"\n",
    "\n",
    "Assuming simplified TF-IDF values:\n",
    "\n",
    "| Document | I    | love | NLP   | machine | learning |\n",
    "|----------|------|------|-------|---------|----------|\n",
    "| Doc 1    | 0.00 | 0.00 | 0.707 | 0.000   | 0.000    |\n",
    "| Doc 2    | 0.00 | 0.00 | 0.000 | 0.577   | 0.577    |\n",
    "\n",
    "🔍 **Interpretation**:\n",
    "- Common words like `\"I\"` and `\"love\"` get a TF-IDF score of 0.\n",
    "- More unique terms like `\"NLP\"`, `\"machine\"`, and `\"learning\"` receive higher weights.\n",
    "\n",
    "##### 🛠️ **Code Example**\n",
    "\n",
    "The code block below uses `TfidfVectorizer` from `sklearn` to generate a TF-IDF matrix and display it using Pandas.\n",
    "\n",
    "This block:\n",
    "- Computes TF-IDF values for all terms in the corpus.\n",
    "- Automatically normalizes and applies the IDF component.\n",
    "- Outputs a readable DataFrame to inspect how word importance varies by document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 5. Word2Vec: Learning Word Meaning Through Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a neural network-based method for learning word embeddings, developed by Google in 2013. It is based on the idea that words used in similar contexts tend to have similar meanings.\n",
    "There are two main architectures:\n",
    "- **Skip-gram**: predicts context words given a center word.\n",
    "- **CBOW (Continuous Bag of Words)**: predicts the center word given the surrounding context.\n",
    "\n",
    "##### **Skip-gram Training Example**\n",
    "\n",
    "In the sentence: *\"I like drinking coffee in the morning\"*, with a context window of 2, if \"drinking\" is the center word, the model will try to predict: [\"I\", \"like\", \"coffee\", \"in\"].\n",
    "\n",
    "Each training step processes one (center, context) pair.\n",
    "\n",
    "The **Skip-gram** model in Word2Vec is based on a simple yet powerful idea: given a central word in a sentence (called the *center word*), the model learns to predict the words that appear around it (called *context words*). For each word in a sentence, a **context window** is defined, typically including a few words to the left and right. The training data consists of many (center, context) pairs extracted from the text.\n",
    "\n",
    "For example, in the sentence:\n",
    "\n",
    "> *\"I like drinking coffee in the morning\"*\n",
    "\n",
    "with a context window of size 2, the center word `\"drinking\"` generates the following context words:\n",
    "\n",
    "> `[\"I\", \"like\", \"coffee\", \"in\"]`\n",
    "\n",
    "So we generate the training pairs:\n",
    "\n",
    "```\n",
    "(\"drinking\", \"I\")\n",
    "(\"drinking\", \"like\")\n",
    "(\"drinking\", \"coffee\")\n",
    "(\"drinking\", \"in\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### Visual Diagram: Skip-gram Training\n",
    "\n",
    "```\n",
    "Context window = 2\n",
    "\n",
    "       Input (center)       Output (context)\n",
    "       ---------------       ----------------\n",
    "             \"drinking\"  →     \"I\"\n",
    "             \"drinking\"  →     \"like\"\n",
    "             \"drinking\"  →     \"coffee\"\n",
    "             \"drinking\"  →     \"in\"\n",
    "```\n",
    "\n",
    "Each training step processes one (center, context) pair at a time. The input is represented as a **one-hot vector**, which is passed through a shallow two-layer neural network. The output layer predicts a probability distribution over the entire vocabulary. The model is trained to give high probability to the actual context word, and low probability to unrelated ones.\n",
    "\n",
    "To avoid computing the full softmax over the entire vocabulary (which can be huge), Word2Vec uses a technique called **negative sampling**. Instead of updating weights for all output words, it only updates those for the correct context word and a small number of randomly selected \"negative\" words.\n",
    "\n",
    "Once training is complete, the learned vectors (from the input weight matrix) are used as **word embeddings** — dense, semantic vector representations of words. In this space, words with similar meanings are located close to each other.\n",
    "\n",
    "---\n",
    "\n",
    "##### 📋 Table: Pairs Generated from a Sentence\n",
    "\n",
    "Let’s take the sentence:\n",
    "\n",
    "> *\"I like coffee very much\"*\n",
    "\n",
    "With a context window size of 1, the Skip-gram model generates the following (center, context) pairs:\n",
    "\n",
    "| Center Word | Context Word |\n",
    "|-------------|---------------|\n",
    "| I           | like          |\n",
    "| like        | I             |\n",
    "| like        | coffee        |\n",
    "| coffee      | like          |\n",
    "| coffee      | very          |\n",
    "| very        | coffee        |\n",
    "| very        | much          |\n",
    "| much        | very          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained word embeddings (GloVe)\n",
    "print(\"Loading Word2Vec word embeddings...\")\n",
    "word2vec_vectors = load_or_download_embedding(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of word vectors in the model:\", len(word2vec_vectors))\n",
    "print(\"Dimension of each word vector:\", word2vec_vectors.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example embeddings for 'deep' and 'learning'\n",
    "print(\"Embedding for 'deep' (first 10 dimensions):\", word2vec_vectors['deep'][:10], \"...\")\n",
    "print(\"Embedding for 'learning' (first 10 dimensions):\", word2vec_vectors['learning'][:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most similar words to \"learning\"\n",
    "similar_words = word2vec_vectors.most_similar(\"learning\", topn=10)\n",
    "\n",
    "# Print results\n",
    "print(\"Most similar words to 'learning':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why Word2Vec Works\n",
    "\n",
    "- It captures semantic relationships: *king - man + woman ≈ queen*\n",
    "- Learns meaning from context rather than frequency\n",
    "- Efficient to train with Negative Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 6. GloVe (Global Vectors for Word Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is a neural word embedding model that captures **semantic meaning** by learning from global word co-occurrence statistics. Unlike TF-IDF, which produces sparse matrices, GloVe produces dense, low-dimensional vectors where similar words are close in the vector space.\n",
    "\n",
    "##### **What is it?**\n",
    "- GloVe starts by building a **co-occurrence matrix**, where each cell counts how often word *j* appears in the context of word *i*.\n",
    "- It then **factorizes** this matrix so that the **dot product** of word vectors approximates the **log of their co-occurrence**.\n",
    "- This allows the model to capture meaningful relationships between words, including analogies like:\n",
    "  > `\"king\" - \"man\" + \"woman\" ≈ \"queen\"`\n",
    "\n",
    "##### **Formula (Simplified)**\n",
    "\n",
    "The GloVe model learns word vectors such that:\n",
    "\n",
    "> **w<sub>i</sub> · w<sub>j</sub> + b<sub>i</sub> + b<sub>j</sub> ≈ log(X<sub>ij</sub>)**\n",
    "\n",
    "Where:\n",
    "- *w<sub>i</sub>* and *w<sub>j</sub>* are the word vectors for word *i* and context word *j*\n",
    "- *X<sub>ij</sub>* is the number of times word *j* appears in the context of word *i*\n",
    "- *b<sub>i</sub>*, *b<sub>j</sub>* are bias terms\n",
    "- The model minimizes the weighted squared error between both sides\n",
    "\n",
    "##### Imagine a simplified co-occurrence matrix:\n",
    "\n",
    "|         | ice | steam | solid | gas |\n",
    "|---------|-----|--------|--------|-----|\n",
    "| **ice**   |  0  |   3    |   15   |  7  |\n",
    "| **steam** |  3  |   0    |   2    | 13  |\n",
    "\n",
    "\n",
    "##### 📋 GloVe Table Example\n",
    "\n",
    "| Word Pair     | Co-occurrence | log(X<sub>ij</sub>) | GloVe dot product |\n",
    "|---------------|----------------|---------------------|-------------------|\n",
    "| ice, solid    | 15             | ~2.71               | close to 2.71     |\n",
    "| ice, gas      | 7              | ~1.95               | close to 1.95     |\n",
    "| steam, solid  | 2              | ~0.69               | close to 0.69     |\n",
    "| steam, gas    | 13             | ~2.56               | close to 2.56     |\n",
    "\n",
    "- GloVe uses these co-occurrence counts (or smoothed versions) to learn word embeddings.\n",
    "- It trains word vectors so that their dot product approximates the **logarithm** of the number of times the words co-occur.\n",
    "\n",
    "For example:\n",
    "- `dot(ice, solid) ≈ log(15)`\n",
    "- `dot(ice, gas) ≈ log(7)`\n",
    "- `dot(steam, solid) ≈ log(2)`\n",
    "- `dot(steam, gas) ≈ log(13)`\n",
    "\n",
    "This training process helps the model learn **meaningful differences** between words:\n",
    "- “ice” is more strongly associated with “solid” than “gas”\n",
    "- “steam” is more strongly associated with “gas” than “solid”\n",
    "\n",
    "✅ The result is that similar words end up with similar vectors, and **vector differences** can capture relationships and analogies.\n",
    "\n",
    "\n",
    "##### 🛠️ **Code Example**\n",
    "\n",
    "The code block below loads the `\"glove-wiki-gigaword-50\"` model and explores:\n",
    "- The shape and dimension of the vectors\n",
    "- Examples of real embeddings (e.g., `\"ice\"` and `\"steam\"`)\n",
    "- Arithmetic on vectors to reveal patterns (e.g., plural forms, analogies)\n",
    "\n",
    "These embeddings can be used as input features for tasks like clustering, topic modeling, or classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained word embeddings (GloVe)\n",
    "print(\"Loading GloVe word embeddings...\")\n",
    "glove_vectors = load_or_download_embedding(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of word vectors in the model:\", len(glove_vectors))\n",
    "print(\"Dimension of each word vector:\", glove_vectors.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example embeddings for 'deep' and 'learning'\n",
    "print(\"Embedding for 'deep': \", glove_vectors['deep'][:10], \"...\")\n",
    "print(\" Embedding for 'learnig': \", glove_vectors['learning'][:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Extract Word Embeddings from Corpus\n",
    "unique_words = list(set(word_tokenize(\" \".join(documents).lower())))\n",
    "word_embeddings = np.array([get_embedding(word, glove_vectors) for word in unique_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = word_embeddings.shape\n",
    "print(\"Number of words:\", rows)\n",
    "print(\"Embedding dimensions:\", cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
