{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a simple test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"I love deep learning and natural language processing.\",\n",
    "    \"Natural language models are fascinating.\",\n",
    "    \"Topic modeling helps to discover themes in documents.\",\n",
    "    \"Machine learning enables automatic topic discovery.\",\n",
    "    \"Neural networks learn embeddings from data.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bag-of-Words (BoW) Representation\n",
    "\n",
    "The **Bag-of-Words** model is one of the simplest ways to represent text numerically. It ignores grammar and word order and focuses only on word occurrence.\n",
    "\n",
    "##### **What is it?**\n",
    "- Each document is treated as a \"bag\" of individual words.\n",
    "- A vocabulary is built from all the unique words in the corpus.\n",
    "- Each document is then represented as a vector counting how many times each word from the vocabulary appears.\n",
    "\n",
    "This results in a **document-term matrix**:\n",
    "- Each row corresponds to a document.\n",
    "- Each column corresponds to a word from the vocabulary.\n",
    "- Each cell contains the count of the word in that document.\n",
    "\n",
    "Although simple, BoW has limitations:\n",
    "- It does not consider word order or context.\n",
    "- It can result in very high-dimensional and sparse data.\n",
    "\n",
    "##### **Simple Example**\n",
    "Let's say we have two short documents:\n",
    "\n",
    "- Document 1: \"I love NLP\"\n",
    "- Document 2: \"I love machine learning\"\n",
    "\n",
    "The combined vocabulary is: `[I, love, NLP, machine, learning]`\n",
    "\n",
    "We can represent each document as a vector of word counts:\n",
    "\n",
    "| Document | I | love | NLP | machine | learning |\n",
    "|----------|---|------|-----|---------|----------|\n",
    "| Doc 1    | 1 | 1    | 1   | 0       | 0        |\n",
    "| Doc 2    | 1 | 1    | 0   | 1       | 1        |\n",
    "\n",
    "This matrix shows how many times each word appears in each document. No word order is preserved.\n",
    "\n",
    "Still, it’s a foundational method and helps build intuition for more sophisticated approaches like TF-IDF and word embeddings.\n",
    "\n",
    "##### 🛠️ **Code Example**\n",
    "\n",
    "The code block below uses `CountVectorizer` from `sklearn` to create the BoW matrix and displays it as a Pandas DataFrame for readability.\n",
    "\n",
    "This block creates a Bag-of-Words (BoW) representation of our corpus:\n",
    "- CountVectorizer transforms the documents into a matrix (documents x words).\n",
    "- Each element in the matrix represents how many times a word appears in a document.\n",
    "- The 'fit_transform' function builds the vocabulary and generates the counts.\n",
    "- Finally, we convert the matrix into a Pandas DataFrame for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(documents)\n",
    "pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TF-IDF (Term Frequency–Inverse Document Frequency) Representation\n",
    "\n",
    "TF-IDF is an improvement over the Bag-of-Words model. While BoW only counts word frequency, **TF-IDF balances frequency with uniqueness**, reducing the weight of common words that appear in many documents.\n",
    "\n",
    "##### **What is it?**\n",
    "- **Term Frequency (TF)** measures how often a word appears in a specific document.\n",
    "- **Inverse Document Frequency (IDF)** downweights words that appear in many documents.\n",
    "- The product **TF × IDF** gives more importance to words that are frequent in a document but rare across the corpus.\n",
    "\n",
    "The result is a **weighted document-term matrix** that emphasizes more informative words.\n",
    "\n",
    "TF-IDF(w, d, D) = TF(w, d) × IDF(w, D)\n",
    "\n",
    "Where:\n",
    "- **TF(w, d)** is the term frequency of word *w* in document *d*:\n",
    "  > TF(w, d) = (Number of times *w* appears in *d*) / (Total words in *d*)\n",
    "\n",
    "- **IDF(w, D)** is the inverse document frequency of *w* in the full corpus *D*:\n",
    "  > IDF(w, D) = log[(1 + N) / (1 + DF(w))] + 1\n",
    "\n",
    "  where:\n",
    "  - *N* is the total number of documents\n",
    "  - *DF(w)* is the number of documents containing the word *w*\n",
    "\n",
    "📌 This helps to penalize very common words (like \"the\", \"and\", \"is\") and give more weight to words that are specific to a document.\n",
    "\n",
    "##### 📋 TF-IDF Table Example\n",
    "\n",
    "Let’s use the same two documents:\n",
    "\n",
    "- Document 1: \"I love NLP\"  \n",
    "- Document 2: \"I love machine learning\"\n",
    "\n",
    "Assuming simplified TF-IDF values:\n",
    "\n",
    "| Document | I    | love | NLP   | machine | learning |\n",
    "|----------|------|------|-------|---------|----------|\n",
    "| Doc 1    | 0.00 | 0.00 | 0.707 | 0.000   | 0.000    |\n",
    "| Doc 2    | 0.00 | 0.00 | 0.000 | 0.577   | 0.577    |\n",
    "\n",
    "🔍 **Interpretation**:\n",
    "- Common words like `\"I\"` and `\"love\"` get a TF-IDF score of 0.\n",
    "- More unique terms like `\"NLP\"`, `\"machine\"`, and `\"learning\"` receive higher weights.\n",
    "\n",
    "##### 🛠️ **Code Example**\n",
    "\n",
    "The code block below uses `TfidfVectorizer` from `sklearn` to generate a TF-IDF matrix and display it using Pandas.\n",
    "\n",
    "This block:\n",
    "- Computes TF-IDF values for all terms in the corpus.\n",
    "- Automatically normalizes and applies the IDF component.\n",
    "- Outputs a readable DataFrame to inspect how word importance varies by document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧠 5. GloVe (Global Vectors for Word Representation)\n",
    "\n",
    "GloVe is a neural word embedding model that captures **semantic meaning** by learning from global word co-occurrence statistics. Unlike TF-IDF, which produces sparse matrices, GloVe produces dense, low-dimensional vectors where similar words are close in the vector space.\n",
    "\n",
    "##### **What is it?**\n",
    "- GloVe starts by building a **co-occurrence matrix**, where each cell counts how often word *j* appears in the context of word *i*.\n",
    "- It then **factorizes** this matrix so that the **dot product** of word vectors approximates the **log of their co-occurrence**.\n",
    "- This allows the model to capture meaningful relationships between words, including analogies like:\n",
    "  > `\"king\" - \"man\" + \"woman\" ≈ \"queen\"`\n",
    "\n",
    "##### **Formula (Simplified)**\n",
    "\n",
    "The GloVe model learns word vectors such that:\n",
    "\n",
    "> **w<sub>i</sub> · w<sub>j</sub> + b<sub>i</sub> + b<sub>j</sub> ≈ log(X<sub>ij</sub>)**\n",
    "\n",
    "Where:\n",
    "- *w<sub>i</sub>* and *w<sub>j</sub>* are the word vectors for word *i* and context word *j*\n",
    "- *X<sub>ij</sub>* is the number of times word *j* appears in the context of word *i*\n",
    "- *b<sub>i</sub>*, *b<sub>j</sub>* are bias terms\n",
    "- The model minimizes the weighted squared error between both sides\n",
    "\n",
    "##### Imagine a simplified co-occurrence matrix:\n",
    "\n",
    "|         | ice | steam | solid | gas |\n",
    "|---------|-----|--------|--------|-----|\n",
    "| **ice**   |  0  |   3    |   15   |  7  |\n",
    "| **steam** |  3  |   0    |   2    | 13  |\n",
    "\n",
    "\n",
    "##### 📋 GloVe Table Example\n",
    "\n",
    "| Word Pair     | Co-occurrence | log(X<sub>ij</sub>) | GloVe dot product |\n",
    "|---------------|----------------|---------------------|-------------------|\n",
    "| ice, solid    | 15             | ~2.71               | close to 2.71     |\n",
    "| ice, gas      | 7              | ~1.95               | close to 1.95     |\n",
    "| steam, solid  | 2              | ~0.69               | close to 0.69     |\n",
    "| steam, gas    | 13             | ~2.56               | close to 2.56     |\n",
    "\n",
    "- GloVe uses these co-occurrence counts (or smoothed versions) to learn word embeddings.\n",
    "- It trains word vectors so that their dot product approximates the **logarithm** of the number of times the words co-occur.\n",
    "\n",
    "For example:\n",
    "- `dot(ice, solid) ≈ log(15)`\n",
    "- `dot(ice, gas) ≈ log(7)`\n",
    "- `dot(steam, solid) ≈ log(2)`\n",
    "- `dot(steam, gas) ≈ log(13)`\n",
    "\n",
    "This training process helps the model learn **meaningful differences** between words:\n",
    "- “ice” is more strongly associated with “solid” than “gas”\n",
    "- “steam” is more strongly associated with “gas” than “solid”\n",
    "\n",
    "✅ The result is that similar words end up with similar vectors, and **vector differences** can capture relationships and analogies.\n",
    "\n",
    "\n",
    "##### 🛠️ **Code Example**\n",
    "\n",
    "The code block below loads the `\"glove-wiki-gigaword-50\"` model and explores:\n",
    "- The shape and dimension of the vectors\n",
    "- Examples of real embeddings (e.g., `\"ice\"` and `\"steam\"`)\n",
    "- Arithmetic on vectors to reveal patterns (e.g., plural forms, analogies)\n",
    "\n",
    "These embeddings can be used as input features for tasks like clustering, topic modeling, or classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained word embeddings (GloVe)\n",
    "print(\"Loading GloVe word embeddings...\")\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of word vectors in the model:\", len(glove_vectors))\n",
    "print(\"Dimension of each word vector:\", glove_vectors.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example embeddings for 'deep' and 'learning'\n",
    "print(\"Embedding for 'deep': \", glove_vectors['deep'][:10], \"...\")\n",
    "print(\" Embedding for 'learnig': \", glove_vectors['learning'][:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define Helper to Retrieve Embeddings\n",
    "def get_embedding(word):\n",
    "    if word in glove_vectors:\n",
    "        return glove_vectors[word]\n",
    "    else:\n",
    "        return np.zeros(50)  # Return a zero vector for unknown words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Extract Word Embeddings from Corpus\n",
    "unique_words = list(set(word_tokenize(\" \".join(documents).lower())))\n",
    "word_embeddings = np.array([get_embedding(word) for word in unique_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 30\n",
      "Embedding dimensions: 50\n"
     ]
    }
   ],
   "source": [
    "rows, cols = word_embeddings.shape\n",
    "print(\"Number of words:\", rows)\n",
    "print(\"Embedding dimensions:\", cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
