{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Sentiment Analysis (Italian Tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bs4 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset directly from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# Load the CHANGE-IT dataset from Hugging Face\n",
    "# dataset = load_dataset(\"gsarti/change_it\", split=\"train\")\n",
    "\n",
    "# Convert Hugging Face dataset to Pandas DataFrame\n",
    "# df = dataset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset directly from Data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I generate the datasets sampling change-it datasets (you don't need to run this code)\n",
    "\n",
    "# Load datasets\n",
    "# df_repubblica = pd.read_csv(\"change-it/change-it.repubblica.train.csv\", sep=',')\n",
    "# df_ilgiornale = pd.read_csv(\"change-it/change-it.ilgiornale.train.csv\", sep=',')\n",
    "\n",
    "# Estract a 1% sample\n",
    "# df_repubblica_sample = df_repubblica.sample(frac=0.01, random_state=42)\n",
    "# df_ilgiornale_sample = df_ilgiornale.sample(frac=0.01, random_state=42)\n",
    "\n",
    "# Salva the sample\n",
    "# df_repubblica_sample.to_csv(\"data/repubblica_sample.csv\", index=False)\n",
    "# df_ilgiornale_sample.to_csv(\"data/ilgiornale_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/repubblica_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Francia, treno travolge scuolabus: cinque stud...</td>\n",
       "      <td>PARIGI - Uno scuolabus √® stato travolto da un ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forte Braschi, cos√¨ gli 007 spiavano durante l...</td>\n",
       "      <td>ROMA - In una galleria sotterranea dell'ottoce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Federico Motta: \"Vogliamo tornare a essere pro...</td>\n",
       "      <td>ROMA . \"Sono gli editori che vogliono e devono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unioni civili, l'ultima sfida di Renzi: \"Ho ri...</td>\n",
       "      <td>ROMA . \"La faccia ce l'ho messa. Anzi, ho fatt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert Armstrong del Mit: \"Siamo vicini al sol...</td>\n",
       "      <td>\"Abbiamo due sfide davanti a noi: de-carbonizz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>Riforme, sul Senato il governo fa l'accordo co...</td>\n",
       "      <td>ROMA - Sulle riforme, Matteo Renzi mette in ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>Recuperare l'energia dalle frenate come in F1,...</td>\n",
       "      <td>TORINO - Se il treno arriva in ritardo, almeno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>‚ÄúSalvate i delfini‚Äù: la figlia di Jfk sfida il...</td>\n",
       "      <td>NEW YORK - Il grande telo blu messo come prote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>Stazione spaziale fuori controllo, per la Prot...</td>\n",
       "      <td>ROMA - Ora dopo ora i dati consentono stime pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>Olanda-Cile 2-0: Van Gaal fa filotto, la Roja ...</td>\n",
       "      <td>SAN PAOLO ‚Äì Nel girone risultato fatale alla S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>637 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              headline  \\\n",
       "0    Francia, treno travolge scuolabus: cinque stud...   \n",
       "1    Forte Braschi, cos√¨ gli 007 spiavano durante l...   \n",
       "2    Federico Motta: \"Vogliamo tornare a essere pro...   \n",
       "3    Unioni civili, l'ultima sfida di Renzi: \"Ho ri...   \n",
       "4    Robert Armstrong del Mit: \"Siamo vicini al sol...   \n",
       "..                                                 ...   \n",
       "632  Riforme, sul Senato il governo fa l'accordo co...   \n",
       "633  Recuperare l'energia dalle frenate come in F1,...   \n",
       "634  ‚ÄúSalvate i delfini‚Äù: la figlia di Jfk sfida il...   \n",
       "635  Stazione spaziale fuori controllo, per la Prot...   \n",
       "636  Olanda-Cile 2-0: Van Gaal fa filotto, la Roja ...   \n",
       "\n",
       "                                             full_text  \n",
       "0    PARIGI - Uno scuolabus √® stato travolto da un ...  \n",
       "1    ROMA - In una galleria sotterranea dell'ottoce...  \n",
       "2    ROMA . \"Sono gli editori che vogliono e devono...  \n",
       "3    ROMA . \"La faccia ce l'ho messa. Anzi, ho fatt...  \n",
       "4    \"Abbiamo due sfide davanti a noi: de-carbonizz...  \n",
       "..                                                 ...  \n",
       "632  ROMA - Sulle riforme, Matteo Renzi mette in ta...  \n",
       "633  TORINO - Se il treno arriva in ritardo, almeno...  \n",
       "634  NEW YORK - Il grande telo blu messo come prote...  \n",
       "635  ROMA - Ora dopo ora i dati consentono stime pi...  \n",
       "636  SAN PAOLO ‚Äì Nel girone risultato fatale alla S...  \n",
       "\n",
       "[637 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Italian stopwords\n",
    "stop_words = set(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning (without stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_no_stemming(text):\n",
    "    # 1. Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # 2. Remove URLs and email addresses\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # 3. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 4. Remove punctuation (keep apostrophes and dashes)\n",
    "    text = re.sub(r\"[^\\w\\s'-]\", '', text)\n",
    "\n",
    "    # 5. Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 6. Tokenize and remove stopwords\n",
    "    tokens = nltk.word_tokenize(text, language=\"italian\")\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning (stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create italian stemmer\n",
    "stemmer = SnowballStemmer(\"italian\")\n",
    "\n",
    "def apply_stemming(text):\n",
    "    tokens = text.split()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try methods on a sample sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I giornalisti stavano scrivendo articoli molto interessanti sull'economia italiana.\"\n",
    "\n",
    "# Fase 1: pulizia\n",
    "cleaned = clean_text_no_stemming(sample_text)\n",
    "print(\"Testo pulito:\", cleaned)\n",
    "\n",
    "# Fase 2: stemming\n",
    "stemmed = apply_stemming(cleaned)\n",
    "print(\"Dopo stemming:\", stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply text cleaning to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text cleaning\n",
    "df['cleaned_text'] = df['full_text'].apply(clean_text_no_stemming)\n",
    "df['stemmed_text'] = df['cleaned_text'].apply(apply_stemming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Optional: remove most/least frequent words ---\n",
    "\n",
    "# Get word frequency across all tweets\n",
    "all_words = ' '.join(df['cleaned_text']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Identify top 10 most common and least common words\n",
    "most_common = set([word for word, freq in word_freq.most_common(10)])\n",
    "least_common = set([word for word, freq in word_freq.most_common()[-10:]])\n",
    "\n",
    "# Function to remove frequent and rare words\n",
    "def remove_common_rare_words(text):\n",
    "    tokens = text.split()\n",
    "    return ' '.join([word for word in tokens if word not in most_common and word not in least_common])\n",
    "\n",
    "# Apply additional cleaning step\n",
    "df['final_text'] = df['cleaned_text'].apply(remove_common_rare_words)\n",
    "\n",
    "# Show result\n",
    "print(df[['full_text', 'cleaned_text', 'final_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all cleaned texts into a single list of words\n",
    "all_words = ' '.join(df['cleaned_text']).split()\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Show the 10 most frequent words\n",
    "print(\"Most frequent words:\")\n",
    "print(word_freq.most_common(10))\n",
    "\n",
    "# Show the 10 least frequent words\n",
    "print(\"\\nLeast frequent words:\")\n",
    "print(word_freq.most_common()[-10:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìä Visualize word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 10 most frequent words and their counts\n",
    "common_words, common_counts = zip(*word_freq.most_common(10))\n",
    "\n",
    "# Plot the word frequencies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(common_words, common_counts)\n",
    "plt.title(\"Top 10 Most Frequent Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßπ Define frequent and rare word sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sets of most and least frequent words\n",
    "most_common = set([word for word in word_freq.most_common(10)])\n",
    "least_common = set([word for word in word_freq.most_common()[-10:]])\n",
    "\n",
    "print(\"Words to remove (most frequent):\", most_common)\n",
    "print(\"Words to remove (least frequent):\", least_common)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üõ†Ô∏è Define the filtering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove both most and least frequent words from a text\n",
    "def remove_common_rare_words(text):\n",
    "    tokens = text.split()\n",
    "    return ' '.join([word for word in tokens if word not in most_common and word not in least_common])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Apply filtering and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filtering function to the cleaned texts\n",
    "df['final_text'] = df['cleaned_text'].apply(remove_common_rare_words)\n",
    "\n",
    "# Show comparison between original, cleaned, and final versions\n",
    "df[['full_text', 'cleaned_text', 'final_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òÅÔ∏è Generate word clouds (before and after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create word cloud from cleaned_text (before filtering)\n",
    "text_before = ' '.join(df['cleaned_text'])\n",
    "wordcloud_before = WordCloud(width=800, height=400, background_color='white').generate(text_before)\n",
    "\n",
    "# Create word cloud from final_text (after filtering)\n",
    "text_after = ' '.join(df['final_text'])\n",
    "wordcloud_after = WordCloud(width=800, height=400, background_color='white').generate(text_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üñºÔ∏è Display the word clouds side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display side-by-side comparison\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Before\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud_before, interpolation='bilinear')\n",
    "plt.title('Before Removing Common/Rare Words')\n",
    "plt.axis('off')\n",
    "\n",
    "# After\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloud_after, interpolation='bilinear')\n",
    "plt.title('After Removing Common/Rare Words')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
