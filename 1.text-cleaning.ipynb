{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Italian Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset from Hugging Face (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# Load the CHANGE-IT dataset from Hugging Face\n",
    "# dataset = load_dataset(\"gsarti/change_it\", split=\"train\")\n",
    "\n",
    "# Convert Hugging Face dataset to Pandas DataFrame\n",
    "# df = dataset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample **change-it** public dataset (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I generate the datasets sampling change-it datasets (you don't need to run this code)\n",
    "\n",
    "# Load datasets\n",
    "# df_repubblica = pd.read_csv(\"change-it/change-it.repubblica.train.csv\", sep=',')\n",
    "# df_ilgiornale = pd.read_csv(\"change-it/change-it.ilgiornale.train.csv\", sep=',')\n",
    "\n",
    "# Estract a 1% sample\n",
    "# df_repubblica_sample = df_repubblica.sample(frac=0.01, random_state=42)\n",
    "# df_ilgiornale_sample = df_ilgiornale.sample(frac=0.01, random_state=42)\n",
    "\n",
    "# Salva the sample\n",
    "# df_repubblica_sample.to_csv(\"data/repubblica_sample.csv\", index=False)\n",
    "# df_ilgiornale_sample.to_csv(\"data/ilgiornale_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset (stored in **data** folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/repubblica_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Italian stopwords\n",
    "stop_words = set(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning (without stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_no_stemming(text):\n",
    "    # 1. Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # 2. Remove URLs and email addresses\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # 3. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 4. Remove punctuation (keep apostrophes and dashes)\n",
    "    text = re.sub(r\"[^\\w\\s'-]\", '', text)\n",
    "\n",
    "    # 5. Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 6. Tokenize and remove stopwords\n",
    "    tokens = nltk.word_tokenize(text, language=\"italian\")\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning (stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create italian stemmer\n",
    "stemmer = SnowballStemmer(\"italian\")\n",
    "\n",
    "def apply_stemming(text):\n",
    "    tokens = text.split()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try methods on a sample sentence (playground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I giornalisti stavano scrivendo articoli molto interessanti sull'economia italiana.\"\n",
    "\n",
    "# Fase 1: cleaning\n",
    "cleaned = clean_text_no_stemming(sample_text)\n",
    "print(\"Clean text:\", cleaned)\n",
    "\n",
    "# Fase 2: stemming\n",
    "stemmed = apply_stemming(cleaned)\n",
    "print(\"After stemming:\", stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply text cleaning to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text cleaning\n",
    "df['cleaned_text'] = df['full_text'].apply(clean_text_no_stemming)\n",
    "df['stemmed_text'] = df['cleaned_text'].apply(apply_stemming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Optional: remove most/least frequent words ---\n",
    "\n",
    "# Get word frequency across all tweets\n",
    "all_words = ' '.join(df['cleaned_text']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Identify top 10 most common and least common words\n",
    "most_common = set([word for word, freq in word_freq.most_common(10)])\n",
    "least_common = set([word for word, freq in word_freq.most_common()[-10:]])\n",
    "\n",
    "# Function to remove frequent and rare words\n",
    "def remove_common_rare_words(text):\n",
    "    tokens = text.split()\n",
    "    return ' '.join([word for word in tokens if word not in most_common and word not in least_common])\n",
    "\n",
    "# Apply additional cleaning step\n",
    "df['final_text'] = df['cleaned_text'].apply(remove_common_rare_words)\n",
    "\n",
    "# Show result\n",
    "print(df[['full_text', 'cleaned_text', 'final_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all cleaned texts into a single list of words\n",
    "all_words = ' '.join(df['cleaned_text']).split()\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Show the 10 most frequent words\n",
    "print(\"Most frequent words:\")\n",
    "print(word_freq.most_common(10))\n",
    "\n",
    "# Show the 10 least frequent words\n",
    "print(\"\\nLeast frequent words:\")\n",
    "print(word_freq.most_common()[-10:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìä Visualize word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 10 most frequent words and their counts\n",
    "common_words, common_counts = zip(*word_freq.most_common(10))\n",
    "\n",
    "# Plot the word frequencies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(common_words, common_counts)\n",
    "plt.title(\"Top 10 Most Frequent Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßπ Define frequent and rare word sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sets of most and least frequent words\n",
    "most_common = set([word for word in word_freq.most_common(10)])\n",
    "least_common = set([word for word in word_freq.most_common()[-10:]])\n",
    "\n",
    "print(\"Words to remove (most frequent):\", most_common)\n",
    "print(\"Words to remove (least frequent):\", least_common)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üõ†Ô∏è Define the filtering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove both most and least frequent words from a text\n",
    "def remove_common_rare_words(text):\n",
    "    tokens = text.split()\n",
    "    return ' '.join([word for word in tokens if word not in most_common and word not in least_common])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Apply filtering and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filtering function to the cleaned texts\n",
    "df['final_text'] = df['cleaned_text'].apply(remove_common_rare_words)\n",
    "\n",
    "# Show comparison between original, cleaned, and final versions\n",
    "df[['full_text', 'cleaned_text', 'final_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òÅÔ∏è Generate word clouds (before and after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create word cloud from cleaned_text (before filtering)\n",
    "text_before = ' '.join(df['cleaned_text'])\n",
    "wordcloud_before = WordCloud(width=800, height=400, background_color='white').generate(text_before)\n",
    "\n",
    "# Create word cloud from final_text (after filtering)\n",
    "text_after = ' '.join(df['final_text'])\n",
    "wordcloud_after = WordCloud(width=800, height=400, background_color='white').generate(text_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üñºÔ∏è Display the word clouds side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display side-by-side comparison\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Before\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud_before, interpolation='bilinear')\n",
    "plt.title('Before Removing Common/Rare Words')\n",
    "plt.axis('off')\n",
    "\n",
    "# After\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloud_after, interpolation='bilinear')\n",
    "plt.title('After Removing Common/Rare Words')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
