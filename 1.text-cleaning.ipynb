{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing for Italian Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install bs4 nltk seaborn wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Set a clean visual style\n",
        "sns.set_theme(style=\"whitegrid\", context=\"notebook\")  # or \"talk\" for presentations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load NLTK resources (from the **resources** folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This setup is needed because there is a known bug with **italian** resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add resource folder to path\n",
        "base_dir = os.getcwd()  # use working dir\n",
        "resources_path = os.path.join(base_dir, \"resources\")\n",
        "\n",
        "# Load italian tokenize\n",
        "with open(os.path.join(resources_path, \"italian_py3.pickle\"), 'rb') as f:\n",
        "    italian_tokenizer = pickle.load(f)\n",
        "\n",
        "# Load italian stopwords\n",
        "with open(os.path.join(resources_path, \"stopwords_it.txt\"), 'r', encoding='utf-8') as f:\n",
        "    stop_words = set(line.strip() for line in f if line.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import dataset from Hugging Face (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install datasets\n",
        "\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# Load the CHANGE-IT dataset from Hugging Face\n",
        "# dataset = load_dataset(\"gsarti/change_it\", split=\"train\")\n",
        "\n",
        "# Convert Hugging Face dataset to Pandas DataFrame\n",
        "# df = dataset.to_pandas()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample **change-it** public dataset (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I generate the datasets sampling change-it datasets (you don't need to run this code)\n",
        "\n",
        "# Load datasets\n",
        "# df_repubblica = pd.read_csv(\"change-it/change-it.repubblica.train.csv\", sep=',')\n",
        "# df_ilgiornale = pd.read_csv(\"change-it/change-it.ilgiornale.train.csv\", sep=',')\n",
        "\n",
        "# Estract a 1% sample\n",
        "# df_repubblica_sample = df_repubblica.sample(frac=0.01, random_state=42)\n",
        "# df_ilgiornale_sample = df_ilgiornale.sample(frac=0.01, random_state=42)\n",
        "\n",
        "# Salva the sample\n",
        "# df_repubblica_sample.to_csv(\"data/repubblica_sample.csv\", index=False)\n",
        "# df_ilgiornale_sample.to_csv(\"data/ilgiornale_sample.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset (stored in **data** folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data/repubblica_sample.csv\")\n",
        "\n",
        "# add a column with the newspaper name\n",
        "df['newspaper'] = 'repubblica'\n",
        "\n",
        "# Print df columns\n",
        "print(df.columns)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text cleaning (without stemming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "\n",
        "def clean_text_no_stemming(text):\n",
        "    # 1. Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # 2. Normalize curly quotes and dashes\n",
        "    text = text.replace(\"‚Äô\", \"'\").replace(\"‚Äò\", \"'\") \\\n",
        "               .replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"') \\\n",
        "               .replace(\"‚Äì\", \"-\").replace(\"‚Äî\", \"-\")\n",
        "\n",
        "    # Optional: normalize Unicode characters to ASCII (e.g., √© ‚Üí e)\n",
        "    # text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
        "\n",
        "    # 3. Remove URLs and email addresses\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # 4. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 5. Remove punctuation (keep straight apostrophes and dashes)\n",
        "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
        "\n",
        "    # 6. Remove digits\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 7. Tokenize and remove stopwords\n",
        "    sentences = italian_tokenizer.tokenize(text)\n",
        "    tokens = [word for sent in sentences for word in sent.split()]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text cleaning (stemming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create italian stemmer\n",
        "stemmer = SnowballStemmer(\"italian\")\n",
        "\n",
        "def apply_stemming(text):\n",
        "    tokens = text.split()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Try methods on a sample sentence (playground)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_text = \"I giornalisti stavano scrivendo articoli molto interessanti sull'economia italiana.\"\n",
        "print(\"Clean text:\", sample_text)\n",
        "\n",
        "# Fase 1: cleaning\n",
        "cleaned = clean_text_no_stemming(sample_text)\n",
        "print(\"Clean text:\", cleaned)\n",
        "\n",
        "# Fase 2: stemming\n",
        "stemmed = apply_stemming(cleaned)\n",
        "print(\"After stemming:\", stemmed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply text cleaning to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply text cleaning\n",
        "df['cleaned_text'] = df['full_text'].apply(clean_text_no_stemming)\n",
        "df['stemmed_text'] = df['cleaned_text'].apply(apply_stemming)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute word frequency (using **Counter** method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join all cleaned texts into a single list of words\n",
        "all_words = ' '.join(df['cleaned_text']).split()\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the 10 most frequent words\n",
        "print(\"Most frequent words:\")\n",
        "print(word_freq.most_common(10))\n",
        "\n",
        "# Show the 10 least frequent words\n",
        "print(\"\\nLeast frequent words:\")\n",
        "print(word_freq.most_common()[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üõ†Ô∏è Define a function to plot word frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_top_words(word_freq, title, top_n=10, color='#4C72B0'):\n",
        "    # Get the top N words and their counts\n",
        "    words, counts = zip(*word_freq.most_common(top_n))\n",
        "    \n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = sns.barplot(x=list(words), y=list(counts), color=color)\n",
        "\n",
        "    # Annotate bars with counts\n",
        "    for i, count in enumerate(counts):\n",
        "        bars.text(i, count + max(counts)*0.01, str(count), \n",
        "                  ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Improve aesthetics\n",
        "    plt.title(title, fontsize=16, fontweight='bold')\n",
        "    plt.xlabel(\"Words\", fontsize=12)\n",
        "    plt.ylabel(\"Frequency\", fontsize=12)\n",
        "    plt.xticks(rotation=45, fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply the plot_top_word to before/after word frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Recalculate frequencies\n",
        "freq_before = Counter(' '.join(df['full_text']).split())\n",
        "freq_after = Counter(' '.join(df['cleaned_text']).split())\n",
        "\n",
        "# Plot comparison\n",
        "plot_top_words(freq_before, \"Top 10 Most Frequent Words (Before Filtering)\", color='#1f77b4')\n",
        "plot_top_words(freq_after, \"Top 10 Most Frequent Words (After Filtering)\", color='#ff7f0e')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define frequent and rare word sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sets of most and least frequent words\n",
        "most_common = set([word for word in word_freq.most_common(10)])\n",
        "least_common = set([word for word in word_freq.most_common()[-10:]])\n",
        "\n",
        "print(\"Words to remove (most frequent):\", most_common)\n",
        "print(\"Words to remove (least frequent):\", least_common)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üõ†Ô∏è Define the filtering function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to remove both most and least frequent words from a text\n",
        "def remove_common_rare_words(text):\n",
        "    tokens = text.split()\n",
        "    return ' '.join([word for word in tokens if word not in most_common and word not in least_common])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß™ Apply filtering and compare results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the filtering function to the cleaned texts\n",
        "df['final_text'] = df['cleaned_text'].apply(remove_common_rare_words)\n",
        "\n",
        "# Show comparison between original, cleaned, and final versions\n",
        "df[['full_text', 'cleaned_text', 'final_text']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚òÅÔ∏è Generate word clouds (before and after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create word cloud from cleaned_text (before filtering)\n",
        "text_before = ' '.join(df['full_text'])\n",
        "wordcloud_before = WordCloud(width=800, height=400, background_color='white').generate(text_before)\n",
        "\n",
        "# Create word cloud from final_text (after filtering)\n",
        "text_after = ' '.join(df['final_text'])\n",
        "wordcloud_after = WordCloud(width=800, height=400, background_color='white').generate(text_after)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the figure\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Word cloud before filtering\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(wordcloud_before, interpolation='bilinear')\n",
        "plt.title(\"Word Cloud ‚Äì Before Filtering\", fontsize=14)\n",
        "plt.axis('off')\n",
        "\n",
        "# Word cloud after filtering\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(wordcloud_after, interpolation='bilinear')\n",
        "plt.title(\"Word Cloud ‚Äì After Filtering\", fontsize=14)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "azureml_py310_sdkv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
